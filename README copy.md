# ğŸ¥ Medical RAG Chatbot - Backend API

A complete **Retrieval-Augmented Generation (RAG)** system for medical Q&A that provides accurate, explainable, and citation-backed answers using **100% free and open-source** tools.

## ğŸŒŸ Features

- âœ… **No Paid APIs**: Completely free using Hugging Face models
- ğŸ” **Semantic Search**: FAISS-based vector similarity search
- ğŸ“š **Citation Support**: Every answer includes source citations
- ğŸ§  **Explainable AI**: Reasoning summaries for transparency
- ğŸ’¾ **Query Logging**: SQLite database for tracking queries
- ğŸ“Š **Evaluation Metrics**: RAGAS-inspired evaluation system
- ğŸš€ **FastAPI Backend**: High-performance async API

## ğŸ“‹ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Framework** | Python + FastAPI |
| **Orchestration** | LangChain |
| **Vector Store** | FAISS (local) |
| **Embeddings** | `BAAI/bge-base-en-v1.5` |
| **LLM** | `microsoft/BiomedGPT-LM-7B` (medical-focused) |
| **Fallback LLM** | `mistralai/Mistral-7B-Instruct-v0.3` |
| **Database** | SQLite |
| **PDF Processing** | PyPDF2 |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Server              â”‚
â”‚  (/query, /ingest, /evaluate)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚
    â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector â”‚    â”‚   RAG    â”‚
â”‚ Store  â”‚â—„â”€â”€â”€â”¤ Pipeline â”‚
â”‚(FAISS) â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  LLM Generator â”‚
          â”‚  (BiomedGPT)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### 1. Navigate to Project

```bash
cd hack-a-cure
```

### 2. Run Installation Script

```bash
python install.py
```

This will:
- Check Python version
- Create necessary directories
- Install all dependencies
- Verify dataset

### 3. Alternative: Manual Installation

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 4. Download Models (Optional - will auto-download on first run)

The models will automatically download on first use, but you can pre-download them:

```bash
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('BAAI/bge-base-en-v1.5')"
```

## ğŸš€ Quick Start

### 1. Start the Server

```bash
python server.py
```

Wait for the server to start, then you'll see:
```
Server starting...
Swagger UI: http://localhost:8000/docs
```

### 2. Ingest Medical Documents

The system comes with a dataset in `HackACure-Dataset/Dataset/`.

Use the `/ingest` endpoint:

```bash
curl -X POST "http://localhost:8000/ingest" \
  -H "Content-Type: application/json" \
  -d '{"use_default_dataset": true}'
```

### 3. Query the Chatbot

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the symptoms of myocardial infarction?",
    "top_k": 5
  }'
```

**Response:**
```json
{
  "answer": "Myocardial infarction (heart attack) symptoms include...",
  "citations": ["Cardiology.pdf", "EmergencyMedicine.pdf"],
  "reasoning_summary": "Retrieved 5 relevant documents from: Cardiology.pdf (relevance: 0.89)...",
  "num_retrieved": 5,
  "response_time_ms": 1234.56
}
```

## ğŸ”Œ API Endpoints

### General

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check and status |
| `/stats` | GET | System statistics |

### Data Ingestion

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/ingest` | POST | Ingest documents from directory |
| `/upload` | POST | Upload and ingest files |
| `/reset` | DELETE | Clear vector store |

### Query & Evaluation

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/query` | POST | Ask a medical question |
| `/evaluate` | POST | Evaluate response quality |

## ğŸ“Š API Examples

### Health Check

```bash
curl http://localhost:8000/health
```

### Upload Custom Documents

```bash
curl -X POST "http://localhost:8000/upload" \
  -F "files=@medical_paper.pdf" \
  -F "files=@clinical_notes.txt"
```

### Evaluate Query Quality

```bash
curl -X POST "http://localhost:8000/evaluate" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is diabetes mellitus?",
    "top_k": 5
  }'
```

**Response:**
```json
{
  "query": "What is diabetes mellitus?",
  "faithfulness_score": 0.85,
  "context_recall": 0.78,
  "context_precision": 0.82,
  "answer_relevancy": 0.88
}
```

### Get Statistics

```bash
curl http://localhost:8000/stats
```

## ğŸ“ Project Structure

```
hack-a-cure/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                 # FastAPI application
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ query_log.py            # SQLite query logging
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_loader.py           # PDF text extraction
â”‚   â””â”€â”€ text_splitter.py        # Advanced text chunking
â”œâ”€â”€ HackACure-Dataset/
â”‚   â””â”€â”€ Dataset/                # Medical PDFs (9 domains)
â”‚       â”œâ”€â”€ Cardiology.pdf
â”‚       â”œâ”€â”€ EmergencyMedicine.pdf
â”‚       â””â”€â”€ ...
â”œâ”€â”€ data/                       # Auto-generated data directory
â”‚   â”œâ”€â”€ faiss_index/            # Vector store
â”‚   â”œâ”€â”€ uploads/                # Uploaded files
â”‚   â””â”€â”€ query_logs.db           # Query logs
â”œâ”€â”€ config.py                   # Configuration settings
â”œâ”€â”€ vector_store.py             # FAISS vector store management
â”œâ”€â”€ rag_pipeline.py             # Core RAG logic
â”œâ”€â”€ server.py                   # Server launcher
â”œâ”€â”€ install.py                  # Installation script
â”œâ”€â”€ test_system.py              # System test suite
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Docker configuration
â”œâ”€â”€ docker-compose.yml          # Docker orchestration
â”œâ”€â”€ .gitignore                  # Git ignore rules
â””â”€â”€ README.md                   # This file
```

## âš™ï¸ Configuration

Edit `config.py` to customize:

```python
# Models
EMBEDDING_MODEL = "BAAI/bge-base-en-v1.5"
GENERATOR_MODEL = "microsoft/BiomedGPT-LM-7B"

# Chunking
CHUNK_SIZE = 500  # tokens
CHUNK_OVERLAP = 50

# Retrieval
TOP_K_RETRIEVAL = 5
SIMILARITY_THRESHOLD = 0.5

# Generation
MAX_NEW_TOKENS = 512
TEMPERATURE = 0.7
```

## ğŸ§ª Testing the System

### 1. Run the Server

```bash
python server.py
```

Or with custom settings:

```bash
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 2. Access Interactive Docs

Open your browser to:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### 3. Sample Medical Questions

Try these queries:

```bash
# Cardiology
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the risk factors for heart disease?"}'

# Emergency Medicine
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "How do you treat anaphylactic shock?"}'

# Internal Medicine
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the types of diabetes?"}'
```

## ğŸ¯ RAG Pipeline Flow

1. **Ingestion Phase**
   - Load PDFs/TXT files
   - Split into ~500 token chunks
   - Generate embeddings using `all-MiniLM-L6-v2`
   - Store in FAISS index

2. **Query Phase**
   - Embed user question
   - Retrieve top-k similar chunks from FAISS
   - Filter by similarity threshold
   - Format context with citations

3. **Generation Phase**
   - Construct prompt with context
   - Generate answer using BiomedGPT
   - Extract citations from sources
   - Create reasoning summary

4. **Logging Phase**
   - Store query, answer, citations
   - Calculate response metrics
   - Save to SQLite database

## ğŸ”¬ Evaluation Metrics

The system provides RAGAS-inspired metrics:

- **Faithfulness**: Does the answer align with retrieved context?
- **Context Recall**: How much of the query is covered by context?
- **Context Precision**: How relevant are the retrieved documents?
- **Answer Relevancy**: Is the answer substantial and relevant?

## ğŸš¨ Troubleshooting

### Model Download Issues

If models fail to download:

```bash
# Set Hugging Face cache directory
export HF_HOME=/path/to/large/disk

# Or use fallback model
# In config.py, the system will auto-fallback to Mistral if BiomedGPT fails
```

### Out of Memory

For low-memory systems:

```python
# In rag_pipeline.py, reduce batch size
embeddings = self.embedding_model.encode(
    texts,
    batch_size=8  # Reduce from 32
)

# Or use CPU-only mode
device = "cpu"  # Force CPU in rag_pipeline.py
```

### FAISS Index Errors

```bash
# Clear and rebuild index
curl -X DELETE "http://localhost:8000/reset"
curl -X POST "http://localhost:8000/ingest" \
  -H "Content-Type: application/json" \
  -d '{"use_default_dataset": true}'
```

## ğŸ” Security Notes

- This is a development server. For production:
  - Add authentication (JWT, OAuth)
  - Use HTTPS
  - Add rate limiting
  - Sanitize file uploads
  - Use environment variables for secrets

## ğŸ“ˆ Performance Tips

1. **Use GPU**: Install `faiss-gpu` and `torch` with CUDA support
2. **Batch Processing**: Process multiple queries together
3. **Index Optimization**: For large datasets, use `IndexIVFFlat`
4. **Model Quantization**: Use 4-bit/8-bit quantization for LLM
5. **Caching**: Cache frequent queries

## ğŸ¤ Contributing

This is a hackathon project for medical Q&A. Contributions welcome!

## ğŸ“„ License

MIT License - Feel free to use for educational and research purposes.

## ğŸ™ Acknowledgments

- **Hugging Face**: For free model hosting
- **FAISS**: For efficient vector search
- **LangChain**: For RAG orchestration
- **FastAPI**: For the excellent web framework

## ğŸ§ª Testing

Run the automated test suite:

```bash
python test_system.py
```

This will test all endpoints and verify the system is working correctly.

## ğŸ“ Support & Troubleshooting

For issues, check:
- Server logs: Look for errors in terminal
- `/health` endpoint: Check system status
- `/stats` endpoint: Monitor usage
- Interactive docs: http://localhost:8000/docs

## ğŸ“ Quick Commands

```bash
# Install & setup
python install.py

# Start server
python server.py

# Test system
python test_system.py

# Access API docs
# Open browser: http://localhost:8000/docs
```

---

**Built with â¤ï¸ for HackACure Hackathon**

ğŸŒŸ **Free. Open Source. No API Keys Required.** ğŸŒŸ



